{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fae601",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T23:40:53.587177Z",
     "iopub.status.busy": "2025-08-30T23:40:53.586898Z",
     "iopub.status.idle": "2025-08-30T23:40:57.861028Z",
     "shell.execute_reply": "2025-08-30T23:40:57.860171Z"
    },
    "papermill": {
     "duration": 4.28174,
     "end_time": "2025-08-30T23:40:57.862546",
     "exception": false,
     "start_time": "2025-08-30T23:40:53.580806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f165ca75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T23:40:57.872032Z",
     "iopub.status.busy": "2025-08-30T23:40:57.871691Z",
     "iopub.status.idle": "2025-08-30T23:40:57.878790Z",
     "shell.execute_reply": "2025-08-30T23:40:57.878051Z"
    },
    "papermill": {
     "duration": 0.01304,
     "end_time": "2025-08-30T23:40:57.879904",
     "exception": false,
     "start_time": "2025-08-30T23:40:57.866864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_actions = {'favorite',\n",
    " 'page_view',\n",
    " 'remove',\n",
    " 'review_view',\n",
    " 'to_cart',\n",
    " 'unfavorite',\n",
    " 'view_description'}\n",
    "\n",
    "unique_widgets = {None,\n",
    " 'addressBookMap.addressBookBar',\n",
    " 'addressBookMap.addressChangeProcessor',\n",
    " 'cart.cartSplit',\n",
    " 'cart.cartSplitShort',\n",
    " 'cart.controls',\n",
    " 'cart.sharingCart',\n",
    " 'cart.split',\n",
    " 'cart.total',\n",
    " 'catalog.searchResultsV2',\n",
    " 'catalog.warlockShelfScroll',\n",
    " 'club.articleV2',\n",
    " 'cms.bannerCarousel',\n",
    " 'cms.cellList',\n",
    " 'cms.navigationSlider',\n",
    " 'cms.separator',\n",
    " 'cms.storyYearSummaries',\n",
    " 'cms.tileDataSourceWidget',\n",
    " 'cms.uWidgetObject',\n",
    " 'common.annotation',\n",
    " 'common.curtainNavBar',\n",
    " 'common.islandSeparator',\n",
    " 'common.text',\n",
    " 'csma.orderActions',\n",
    " 'csma.orderDoneButtonBar',\n",
    " 'csma.reorderCanceledShipment',\n",
    " 'csma.sellerProducts',\n",
    " 'csma.shipmentWidget',\n",
    " 'csma.textBlock',\n",
    " 'express.cartButtonPopup',\n",
    " 'express.deliveryWidget',\n",
    " 'express.deliveryWidgetBigOzon',\n",
    " 'express.navigationSlider',\n",
    " 'express.orderItems',\n",
    " 'express.orderItemsPopup',\n",
    " 'favorites.listSelector',\n",
    " 'favorites.searchResultsV2',\n",
    " 'favorites.sharedListSearchResults',\n",
    " 'layout.ghost',\n",
    " 'marketing.bigPromoPDP',\n",
    " 'marketing.hammers',\n",
    " 'marketing.sellerProducts',\n",
    " 'messenger.messenger',\n",
    " 'messenger.webMessenger',\n",
    " 'myProfile.sectionMenu',\n",
    " 'pdp-widget',\n",
    " 'pdp.apparelNavBar',\n",
    " 'pdp.aspectTile',\n",
    " 'pdp.aspectsApparelColor',\n",
    " 'pdp.aspectsApparelOther',\n",
    " 'pdp.aspectsApparelSize',\n",
    " 'pdp.aspectsNoSize',\n",
    " 'pdp.badgeList',\n",
    " 'pdp.brand',\n",
    " 'pdp.characteristics',\n",
    " 'pdp.collections',\n",
    " 'pdp.descriptionAccordion',\n",
    " 'pdp.galleryPreview',\n",
    " 'pdp.helpfulHints',\n",
    " 'pdp.inStock',\n",
    " 'pdp.installmentPurchase',\n",
    " 'pdp.modelParams',\n",
    " 'pdp.navBar',\n",
    " 'pdp.navTitle',\n",
    " 'pdp.outOfStock',\n",
    " 'pdp.price',\n",
    " 'pdp.priceCell',\n",
    " 'pdp.richContent',\n",
    " 'pdp.shareLink',\n",
    " 'pdp.shareWithAddFavorite',\n",
    " 'pdp.textBlock',\n",
    " 'pdp.textDescription',\n",
    " 'pdp.tiles',\n",
    " 'pdp.title',\n",
    " 'pdp.webAddToFavorite',\n",
    " 'pdp.webBrand',\n",
    " 'pdp.webCharacteristics',\n",
    " 'pdp.webCompare',\n",
    " 'pdp.webMobCompare',\n",
    " 'pdp.webMobRichContent',\n",
    " 'pdp.webMobTextDescription',\n",
    " 'pdp.webOutOfStock',\n",
    " 'pdp.webProductMini',\n",
    " 'pdp.webSellerList',\n",
    " 'rpProduct.glueReviewList',\n",
    " 'rpProduct.listReviews',\n",
    " 'rpProduct.pinnedReview',\n",
    " 'rpProduct.singleReview',\n",
    " 'rpProduct.tilesReviewsList',\n",
    " 'rpProduct.ugcCounters',\n",
    " 'rpProduct.userReviews',\n",
    " 'rpProduct.userReviewsList',\n",
    " 'rpProduct.webListReviews',\n",
    " 'rtb.advBanner',\n",
    " 'rtb.advPageStay',\n",
    " 'rtb.advVideoBanner',\n",
    " 'rtb.advVideoBannerMobile',\n",
    " 'shelf.accessoriesShelf',\n",
    " 'shelf.analogLookSimilar',\n",
    " 'shelf.analogShelf',\n",
    " 'shelf.analogShelfFavorites',\n",
    " 'shelf.analogShelfPersonal',\n",
    " 'shelf.analogShelfPersonalPrimary',\n",
    " 'shelf.analogShelfReviews',\n",
    " 'shelf.analogShelfSecondary',\n",
    " 'shelf.analogsShelfReturns',\n",
    " 'shelf.apparelCart',\n",
    " 'shelf.apparelOrders',\n",
    " 'shelf.apparelPersonalFemale',\n",
    " 'shelf.apparelPersonalKids',\n",
    " 'shelf.apparelPersonalMale',\n",
    " 'shelf.apparelPersonalSuggest',\n",
    " 'shelf.bestsellers',\n",
    " 'shelf.buyTogether',\n",
    " 'shelf.cartCheckout',\n",
    " 'shelf.cartShelf',\n",
    " 'shelf.filterInfiniteScroll',\n",
    " 'shelf.freshPersonal',\n",
    " 'shelf.infiniteScroll',\n",
    " 'shelf.infiniteScrollSuggests',\n",
    " 'shelf.kindlyReminder',\n",
    " 'shelf.oosModelVariants',\n",
    " 'shelf.ordersShelf',\n",
    " 'shelf.pdpAccessories',\n",
    " 'shelf.personalCategoryShelf',\n",
    " 'shelf.sellerAnalogs',\n",
    " 'shelf.userCart',\n",
    " 'shelf.userHistory',\n",
    " 'shelf.userOrders',\n",
    " 'shell.promoNavBar',\n",
    " 'sis.mallBrandProducts',\n",
    " 'sis.mallSellerProducts',\n",
    " 'tile.relatedProducts',\n",
    " 'tile.tileGridMobile',\n",
    " 'tile.tileScrollMobile',\n",
    " 'tile.tileShelf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea82216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T23:40:57.888461Z",
     "iopub.status.busy": "2025-08-30T23:40:57.888192Z",
     "iopub.status.idle": "2025-08-30T23:40:57.892946Z",
     "shell.execute_reply": "2025-08-30T23:40:57.892354Z"
    },
    "papermill": {
     "duration": 0.010106,
     "end_time": "2025-08-30T23:40:57.894056",
     "exception": false,
     "start_time": "2025-08-30T23:40:57.883950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "success_predecessors = {\n",
    "    ('to_cart', 'cart.controls'),\n",
    "    ('to_cart', 'cart.total'),\n",
    "    ('to_cart', 'express.cartButtonPopup'),\n",
    "    ('to_cart', 'pdp.webAddToFavorite'),\n",
    "    ('favorite', 'pdp.webAddToFavorite'),\n",
    "    ('view_description', 'pdp.textDescription'),\n",
    "    ('page_view', 'pdp.price')\n",
    "}\n",
    "\n",
    "failure_predecessors = {\n",
    "    ('remove', 'cart.controls'),\n",
    "    ('remove', 'cart.total'),\n",
    "    ('unfavorite', 'favorites.listSelector'),\n",
    "    ('unfavorite', 'pdp.webAddToFavorite'),\n",
    "    ('remove', 'shelf.userCart')\n",
    "}\n",
    "\n",
    "state_map = {}\n",
    "k = 0\n",
    "\n",
    "for action in unique_actions:\n",
    "    for widget in unique_widgets:\n",
    "        state_map[(action, widget)] = k\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e323736b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T23:40:57.902363Z",
     "iopub.status.busy": "2025-08-30T23:40:57.902147Z",
     "iopub.status.idle": "2025-08-30T23:40:57.905710Z",
     "shell.execute_reply": "2025-08-30T23:40:57.905146Z"
    },
    "papermill": {
     "duration": 0.008945,
     "end_time": "2025-08-30T23:40:57.906813",
     "exception": false,
     "start_time": "2025-08-30T23:40:57.897868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "succ_pred = set([state_map[pair] for pair in success_predecessors])\n",
    "fail_pred = set([state_map[pair] for pair in failure_predecessors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5d3eea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T23:40:57.915534Z",
     "iopub.status.busy": "2025-08-30T23:40:57.915184Z",
     "iopub.status.idle": "2025-08-30T23:41:40.159375Z",
     "shell.execute_reply": "2025-08-30T23:41:40.158506Z"
    },
    "papermill": {
     "duration": 42.250276,
     "end_time": "2025-08-30T23:41:40.160945",
     "exception": false,
     "start_time": "2025-08-30T23:40:57.910669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.load(\"/kaggle/input/mochaaaa/item_embeddings.npz\")\n",
    "item_embeds_dict = dict(zip(data['keys'], data['embeddings']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6127159c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T23:41:40.170557Z",
     "iopub.status.busy": "2025-08-30T23:41:40.170004Z",
     "iopub.status.idle": "2025-08-30T23:41:40.275144Z",
     "shell.execute_reply": "2025-08-30T23:41:40.274451Z"
    },
    "papermill": {
     "duration": 0.111044,
     "end_time": "2025-08-30T23:41:40.276291",
     "exception": false,
     "start_time": "2025-08-30T23:41:40.165247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27ba5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T23:41:40.285310Z",
     "iopub.status.busy": "2025-08-30T23:41:40.285016Z",
     "iopub.status.idle": "2025-08-30T23:41:40.320991Z",
     "shell.execute_reply": "2025-08-30T23:41:40.320314Z"
    },
    "papermill": {
     "duration": 0.041814,
     "end_time": "2025-08-30T23:41:40.322151",
     "exception": false,
     "start_time": "2025-08-30T23:41:40.280337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_HOURS_SIN = np.sin(2 * np.pi * np.arange(24) / 24, dtype=np.float32)\n",
    "_HOURS_COS = np.cos(2 * np.pi * np.arange(24) / 24, dtype=np.float32)\n",
    "\n",
    "class UserHistoryDataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_dirs,\n",
    "        target_dirs,\n",
    "        succ_set,\n",
    "        fail_set,\n",
    "        state_mapping,\n",
    "        window_size=500,\n",
    "        stride=250,\n",
    "        history_len_thresh=10,\n",
    "    ):\n",
    "        self.data_dirs = data_dirs if isinstance(data_dirs, list) else [data_dirs]\n",
    "        self.target_dirs_path = target_dirs\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.history_len_thresh = history_len_thresh\n",
    "        self.succ_set = frozenset(succ_set)\n",
    "        self.fail_set = frozenset(fail_set)\n",
    "        self.state_mapping = state_mapping\n",
    "        self._precomputed_features = None\n",
    "\n",
    "    def _precompute_user_data(self, int_file):\n",
    "        try:\n",
    "            int_lf = pl.scan_parquet(int_file).select([\n",
    "                'user_id', 'action_widget', 'action_type', 'item_id', 'timestamp'\n",
    "            ])\n",
    "            \n",
    "            user_ids = int_lf.select(pl.col('user_id').unique()).collect()['user_id']\n",
    "            if len(user_ids) == 0:\n",
    "                return\n",
    "\n",
    "            orders = pl.scan_parquet(self.target_dirs_path).filter(\n",
    "                pl.col('user_id').is_in(user_ids)\n",
    "            ).select([\n",
    "                'user_id', 'created_timestamp', 'last_status_timestamp', \n",
    "                'last_status', 'item_id'\n",
    "            ]).collect()\n",
    "\n",
    "            user_data = int_lf.collect()\n",
    "            \n",
    "            return user_data, orders, user_ids\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preprocessing file {int_file}: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    def _process_user(self, user_id, user_data, orders):\n",
    "        user_history = user_data.filter(pl.col('user_id') == user_id)\n",
    "        if user_history.is_empty():\n",
    "            return None\n",
    "\n",
    "        user_orders = orders.filter(pl.col('user_id') == user_id)\n",
    "        if user_orders.is_empty():\n",
    "            return None\n",
    "\n",
    "        history_len = len(user_history)\n",
    "        if history_len < self.history_len_thresh:\n",
    "            return None\n",
    "\n",
    "        results = []\n",
    "        if history_len < self.window_size:\n",
    "            result = self._extract_target(user_history, user_orders)\n",
    "            if result[0] is not None:\n",
    "                results.append(result)\n",
    "        else:\n",
    "            max_start_idx = history_len - self.window_size\n",
    "            for i in range(0, max_start_idx + 1, self.stride):\n",
    "                window = user_history.slice(i, self.window_size)\n",
    "                result = self._extract_target(window, user_orders)\n",
    "                if result[0] is not None:\n",
    "                    results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _extract_target(self, history_window, user_orders):\n",
    "        try:\n",
    "            start_time = history_window['timestamp'].max()\n",
    "            end_time = start_time + timedelta(days=14)\n",
    "            \n",
    "            target_window = user_orders.filter(\n",
    "                (pl.col('created_timestamp') >= start_time) &\n",
    "                (pl.col('last_status_timestamp') <= end_time)\n",
    "            )\n",
    "            \n",
    "            if target_window.is_empty():\n",
    "                return None, None, None, [], []\n",
    "\n",
    "            delivered_mask = target_window['last_status'] == 'delivered_orders'\n",
    "            canceled_mask = target_window['last_status'] == 'canceled_orders'\n",
    "            \n",
    "            positives = target_window.filter(delivered_mask)['item_id'].to_list()\n",
    "            negatives = target_window.filter(canceled_mask)['item_id'].to_list()\n",
    "\n",
    "            if not positives:\n",
    "                return None, None, None, [], []\n",
    "\n",
    "            features_result = self._build_user_features(history_window)\n",
    "            if features_result is None:\n",
    "                return None, None, None, [], []\n",
    "                \n",
    "            padded_features, attention_mask, failure_items = features_result\n",
    "            all_negative_items = list(set(negatives + failure_items))\n",
    "\n",
    "            history_item_ids = history_window['item_id'].to_list()\n",
    "            \n",
    "            return (padded_features, attention_mask, history_item_ids, positives, all_negative_items)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in _extract_target: {e}\")\n",
    "            return None, None, None, [], []\n",
    "\n",
    "    def _build_user_features(self, user_df):\n",
    "        try:\n",
    "            user_history = user_df.sort('timestamp').select([\n",
    "                pl.col('action_widget').fill_null(\"unknown\"),\n",
    "                pl.col('action_type').fill_null(\"unknown\"),\n",
    "                pl.col('item_id').fill_null(0),\n",
    "                pl.col('timestamp')\n",
    "            ])\n",
    "\n",
    "            widgets = user_history['action_widget'].to_numpy()\n",
    "            actions = user_history['action_type'].to_numpy()\n",
    "            item_ids = user_history['item_id'].to_numpy()\n",
    "            timestamps = user_history['timestamp'].to_numpy().astype(np.int64)\n",
    "            \n",
    "            n = len(user_history)\n",
    "            if n == 0:\n",
    "                return None\n",
    "\n",
    "            states = np.array([self.state_mapping.get((a, w), -1) for a, w in zip(actions, widgets)], dtype=np.int32)\n",
    "            is_success = np.isin(states, self.succ_set)\n",
    "            is_failure = np.isin(states, self.fail_set)\n",
    "            negative_items = np.unique(item_ids[is_failure & (item_ids > 0)]).tolist()\n",
    "            \n",
    "            ts_min = timestamps[0]\n",
    "            time_from_start_min = np.clip((timestamps - ts_min) / 6e10, 0, 1e6).astype(np.float32)\n",
    "            \n",
    "            time_delta_min = np.zeros(n, dtype=np.float32)\n",
    "            if n > 1:\n",
    "                time_delta_min[1:] = np.clip(np.diff(timestamps) / 6e10, -1e6, 1e6)\n",
    "            \n",
    "            same_item_as_prev = np.zeros(n, dtype=np.float32)\n",
    "            same_item_as_prev[1:] = (item_ids[1:] == item_ids[:-1]).astype(np.float32)\n",
    "            \n",
    "            same_state_as_prev = np.zeros(n, dtype=np.float32)\n",
    "            same_state_as_prev[1:] = (states[1:] == states[:-1]).astype(np.float32)\n",
    "            \n",
    "            velocity = np.clip(1.0 / (np.abs(time_delta_min) + 1e-6), 0, 1e6).astype(np.float32)\n",
    "            acceleration = np.zeros(n, dtype=np.float32)\n",
    "            if n > 1:\n",
    "                acceleration[1:] = np.clip(np.diff(velocity), -1e6, 1e6)\n",
    "            \n",
    "            seconds = timestamps // 1_000_000_000\n",
    "            hours = (seconds // 3600) % 24\n",
    "            hour_sin = _HOURS_SIN[hours]\n",
    "            hour_cos = _HOURS_COS[hours]\n",
    "            \n",
    "            cum_success_count = np.cumsum(is_success).astype(np.float32)\n",
    "            cum_failure_count = np.cumsum(is_failure).astype(np.float32)\n",
    "            \n",
    "            window_unique_widgets = np.ones(n, dtype=np.float32)\n",
    "            for i in range(1, min(20, n)):\n",
    "                window_unique_widgets[i] = len(np.unique(widgets[max(0, i-19):i+1]))\n",
    "            \n",
    "            interaction_counts = np.arange(1, n + 1, dtype=np.float32)\n",
    "            success_rate = np.divide(cum_success_count, interaction_counts, \n",
    "                                   out=np.zeros_like(cum_success_count), \n",
    "                                   where=interaction_counts!=0)\n",
    "            failure_rate = np.divide(cum_failure_count, interaction_counts,\n",
    "                                   out=np.zeros_like(cum_failure_count),\n",
    "                                   where=interaction_counts!=0)\n",
    "            \n",
    "            numerical_features = np.column_stack([\n",
    "                time_from_start_min, time_delta_min, velocity, acceleration,\n",
    "                same_item_as_prev, same_state_as_prev,\n",
    "                hour_sin, hour_cos,\n",
    "                cum_success_count, cum_failure_count,\n",
    "                window_unique_widgets,\n",
    "                success_rate, failure_rate\n",
    "            ]).astype(np.float32)\n",
    "            \n",
    "            current_len = numerical_features.shape[0]\n",
    "            target_len = self.window_size\n",
    "            \n",
    "            if current_len > target_len:\n",
    "                numerical_features = numerical_features[:target_len]\n",
    "                attention_mask = np.ones(target_len, dtype=np.float32)\n",
    "            else:\n",
    "                padding_len = target_len - current_len\n",
    "                padding = np.zeros((padding_len, numerical_features.shape[1]), dtype=np.float32)\n",
    "                numerical_features = np.concatenate([padding, numerical_features], axis=0)\n",
    "                attention_mask = np.concatenate([\n",
    "                    np.zeros(padding_len, dtype=np.float32),\n",
    "                    np.ones(current_len, dtype=np.float32)\n",
    "                ])\n",
    "            \n",
    "            return numerical_features, attention_mask, negative_items\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Feature building error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        \n",
    "        if worker_info is None:\n",
    "            files_to_process = self.data_dirs\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            num_files = len(self.data_dirs)\n",
    "            \n",
    "            files_per_worker = (num_files + num_workers - 1) // num_workers\n",
    "            start_idx = worker_id * files_per_worker\n",
    "            end_idx = min(start_idx + files_per_worker, num_files)\n",
    "            files_to_process = self.data_dirs[start_idx:end_idx]\n",
    "\n",
    "        for int_file in files_to_process:\n",
    "            try:\n",
    "                user_data, orders, user_ids = self._precompute_user_data(int_file)\n",
    "                if user_data is None:\n",
    "                    continue\n",
    "                \n",
    "                random.shuffle(user_ids)\n",
    "                \n",
    "                for user_id in user_ids:\n",
    "                    results = self._process_user(user_id, user_data, orders)\n",
    "                    if results:\n",
    "                        for result in results:\n",
    "                            yield result\n",
    "                    \n",
    "                    if random.random() < 0.1:  # 10% chance\n",
    "                        gc.collect()\n",
    "                \n",
    "                del user_data, orders, user_ids\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {int_file}: {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e84fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T23:41:40.331181Z",
     "iopub.status.busy": "2025-08-30T23:41:40.330931Z",
     "iopub.status.idle": "2025-08-30T23:41:40.338160Z",
     "shell.execute_reply": "2025-08-30T23:41:40.337476Z"
    },
    "papermill": {
     "duration": 0.013066,
     "end_time": "2025-08-30T23:41:40.339289",
     "exception": false,
     "start_time": "2025-08-30T23:41:40.326223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def final_collate_fn(batch):\n",
    "    global item_embeds_dict\n",
    "    try:\n",
    "        padded_features, attention_masks, history_item_ids, pos_item_ids, neg_item_ids = zip(*batch)\n",
    "        dummy_emb = np.zeros_like(next(iter(item_embeds_dict.values())), dtype=np.float32)\n",
    "\n",
    "        def convert_ids_to_embeddings(ids_list):\n",
    "            return [item_embeds_dict.get(id_, dummy_emb) for id_ in ids_list]\n",
    "        \n",
    "        history_embs = [convert_ids_to_embeddings(ids) for ids in history_item_ids]  # embeddings истории\n",
    "        pos_embs = [convert_ids_to_embeddings(ids) for ids in pos_item_ids]          # embeddings позитивов\n",
    "        \n",
    "        # Обработка негативов - добавляем случайные если их мало\n",
    "        processed_neg_embs = []\n",
    "        all_item_ids = list(item_embeds_dict.keys())\n",
    "        \n",
    "        for i, neg_ids in enumerate(neg_item_ids):\n",
    "            neg_embeddings = convert_ids_to_embeddings(neg_ids)\n",
    "            \n",
    "            # Если негативов меньше 5, добавляем случайные\n",
    "            if len(neg_embeddings) < 5:\n",
    "                needed_more = 5 - len(neg_embeddings)\n",
    "                # Берем случайные айтемы, которых нет среди позитивов\n",
    "                pos_ids_set = set(pos_item_ids[i])\n",
    "                available_ids = [item_id for item_id in all_item_ids if item_id not in pos_ids_set]\n",
    "                \n",
    "                if len(available_ids) > 0:\n",
    "                    import random\n",
    "                    random_ids = random.sample(available_ids, min(needed_more, len(available_ids)))\n",
    "                    random_embeddings = convert_ids_to_embeddings(random_ids)\n",
    "                    neg_embeddings.extend(random_embeddings)\n",
    "            \n",
    "            processed_neg_embs.append(neg_embeddings)\n",
    "        \n",
    "        neg_embs = processed_neg_embs\n",
    "    \n",
    "        features_tensor = torch.as_tensor(np.stack(padded_features), dtype=torch.float32)\n",
    "        masks_tensor = torch.as_tensor(np.stack(attention_masks), dtype=torch.float32)\n",
    "        \n",
    "        return features_tensor, masks_tensor, history_embs, pos_embs, neg_embs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in collate_fn: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f7e54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T23:41:40.347921Z",
     "iopub.status.busy": "2025-08-30T23:41:40.347698Z",
     "iopub.status.idle": "2025-08-30T23:41:40.364509Z",
     "shell.execute_reply": "2025-08-30T23:41:40.363739Z"
    },
    "papermill": {
     "duration": 0.022584,
     "end_time": "2025-08-30T23:41:40.365723",
     "exception": false,
     "start_time": "2025-08-30T23:41:40.343139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ItemTower(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=128, dropout_rate=0.2):\n",
    "        super(ItemTower, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, items):\n",
    "        return self.mlp(items)\n",
    "\n",
    "class UserTower(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        item_emb_dim,\n",
    "        user_feature_dim,\n",
    "        gru_hidden_dim=64,\n",
    "        user_output_dim=128,\n",
    "        mlp_hidden_dim=256,\n",
    "        dropout_rate=0.2\n",
    "    ):\n",
    "        super(UserTower, self).__init__()\n",
    "        \n",
    "        self.gru_input_dim = item_emb_dim + user_feature_dim\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.gru_input_dim,\n",
    "            hidden_size=gru_hidden_dim,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if 1 > 1 else 0.0\n",
    "        )\n",
    "        \n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_hidden_dim, user_output_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.output_dim = user_output_dim\n",
    "\n",
    "    def forward(self, history_item_embs, user_other_features, attention_mask):\n",
    "        batch_size, max_seq_len, user_feat_dim = user_other_features.shape\n",
    "        item_emb_dim = history_item_embs[0].shape[-1]\n",
    "        \n",
    "        padded_history_item_embs = []\n",
    "        for i, item_emb_seq in enumerate(history_item_embs):\n",
    "            seq_len = item_emb_seq.shape[0]\n",
    "            if seq_len < max_seq_len:\n",
    "                padding = torch.zeros((max_seq_len - seq_len, item_emb_dim), device=item_emb_seq.device)\n",
    "                padded_seq = torch.cat([item_emb_seq, padding], dim=0)\n",
    "            else:\n",
    "                padded_seq = item_emb_seq[:max_seq_len]\n",
    "            padded_history_item_embs.append(padded_seq)\n",
    "        \n",
    "        item_embs_tensor = torch.stack(padded_history_item_embs, dim=0)\n",
    "        \n",
    "        combined_features = torch.cat([item_embs_tensor, user_other_features], dim=-1)\n",
    "\n",
    "        lengths = attention_mask.sum(dim=1).long()\n",
    "        lengths = torch.clamp(lengths, min=1)\n",
    "\n",
    "        lengths_sorted, sorted_idx = lengths.sort(descending=True)\n",
    "        combined_features_sorted = combined_features[sorted_idx]\n",
    "        \n",
    "        packed = rnn_utils.pack_padded_sequence(\n",
    "            combined_features_sorted,\n",
    "            lengths_sorted.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=True\n",
    "        )\n",
    "\n",
    "        packed_gru_out, hidden = self.gru(packed)\n",
    "        hidden_sorted = hidden.squeeze(0)\n",
    "\n",
    "        _, original_idx = sorted_idx.sort()\n",
    "        hidden_original = hidden_sorted[original_idx]\n",
    "\n",
    "        user_embedding = self.final_mlp(hidden_original)\n",
    "        \n",
    "        return user_embedding\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        item_input_dim=128,\n",
    "        item_output_dim=128,\n",
    "        user_feature_dim=13,\n",
    "        gru_hidden_dim=64,\n",
    "        user_output_dim=128,\n",
    "        mlp_hidden_dim=256,\n",
    "        dropout_rate=0.2\n",
    "    ):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        \n",
    "        self.item_emb_dim = item_input_dim\n",
    "        \n",
    "        self.user_tower = UserTower(\n",
    "            item_emb_dim=self.item_emb_dim,\n",
    "            user_feature_dim=user_feature_dim,\n",
    "            gru_hidden_dim=gru_hidden_dim,\n",
    "            user_output_dim=user_output_dim,\n",
    "            mlp_hidden_dim=mlp_hidden_dim,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        \n",
    "        self.item_tower = ItemTower(\n",
    "            input_dim=item_input_dim,\n",
    "            output_dim=item_output_dim,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "    def forward(self, padded_features_tensor, attention_masks_tensor, history_item_embs_tensors, pos_items_tensors, neg_items_tensors):\n",
    "        batch_size = padded_features_tensor.shape[0]\n",
    "        \n",
    "        user_emb = self.user_tower(\n",
    "            history_item_embs_tensors,\n",
    "            padded_features_tensor,\n",
    "            attention_masks_tensor\n",
    "        )\n",
    "        \n",
    "        pos_scores = []\n",
    "        neg_scores = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Позитивные\n",
    "            if pos_items_tensors[i].shape[0] > 0:\n",
    "                pos_item_emb = self.item_tower(pos_items_tensors[i])\n",
    "                scores_pos = torch.matmul(user_emb[i], pos_item_emb.T)\n",
    "            else:\n",
    "                scores_pos = torch.tensor([], dtype=torch.float32, device=user_emb.device)\n",
    "            pos_scores.append(scores_pos)\n",
    "\n",
    "            # Негативные\n",
    "            if neg_items_tensors[i].shape[0] > 0:\n",
    "                neg_item_emb = self.item_tower(neg_items_tensors[i])\n",
    "                scores_neg = torch.matmul(user_emb[i], neg_item_emb.T)\n",
    "            else:\n",
    "                scores_neg = torch.tensor([], dtype=torch.float32, device=user_emb.device)\n",
    "            neg_scores.append(scores_neg)\n",
    "        \n",
    "        return pos_scores, neg_scores\n",
    "\n",
    "class PairwiseLogisticLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pos_scores, neg_scores):\n",
    "        total_loss = 0.0\n",
    "        total_pairs = 0\n",
    "        for p_scores, n_scores in zip(pos_scores, neg_scores):\n",
    "            if p_scores.numel() == 0 or n_scores.numel() == 0:\n",
    "                continue\n",
    "            diff = p_scores.unsqueeze(1) - n_scores.unsqueeze(0)\n",
    "            loss_per_pair = torch.log(1 + torch.exp(-diff.clamp(max=10))).clamp(max=10)\n",
    "            total_loss += loss_per_pair.sum()\n",
    "            total_pairs += diff.numel()\n",
    "        return total_loss / (total_pairs + 1e-9) if total_pairs > 0 else torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c059f56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T23:41:40.375542Z",
     "iopub.status.busy": "2025-08-30T23:41:40.375284Z",
     "iopub.status.idle": "2025-08-30T23:41:40.425266Z",
     "shell.execute_reply": "2025-08-30T23:41:40.424468Z"
    },
    "papermill": {
     "duration": 0.056557,
     "end_time": "2025-08-30T23:41:40.426530",
     "exception": false,
     "start_time": "2025-08-30T23:41:40.369973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 50\n",
    "SAVE_PATH = \"./checkpoints\"\n",
    "BEST_MODEL_FILENAME = \"best_model.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af82e73b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T23:41:40.435350Z",
     "iopub.status.busy": "2025-08-30T23:41:40.435108Z",
     "iopub.status.idle": "2025-08-30T23:41:40.449744Z",
     "shell.execute_reply": "2025-08-30T23:41:40.449063Z"
    },
    "papermill": {
     "duration": 0.020086,
     "end_time": "2025-08-30T23:41:40.450869",
     "exception": false,
     "start_time": "2025-08-30T23:41:40.430783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Batch size: 128\n"
     ]
    }
   ],
   "source": [
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c23a1c2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T23:41:40.459660Z",
     "iopub.status.busy": "2025-08-30T23:41:40.459401Z",
     "iopub.status.idle": "2025-08-30T23:41:40.703283Z",
     "shell.execute_reply": "2025-08-30T23:41:40.702689Z"
    },
    "papermill": {
     "duration": 0.249807,
     "end_time": "2025-08-30T23:41:40.704679",
     "exception": false,
     "start_time": "2025-08-30T23:41:40.454872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#paths\n",
    "data_dirs_path1 = \"/kaggle/input/user-chunks-pt2/ultra_mega_user_chunks_pt1\"\n",
    "data_dirs_path2 = \"/kaggle/input/user-chunks-pt1\"\n",
    "target_dirs_path = \"/kaggle/input/ozon-data/ml_ozon_recsys_train_final_apparel_orders_data/ml_ozon_recsys_train_final_apparel_orders_data\"\n",
    "item_embeds_path = \"/kaggle/input/mochaaaa/item_embeddings.npz\"\n",
    "checkpoint_path = \"/kaggle/input/modelka20/pytorch/default/1/checkpoints/checkpoint_batch_2460.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "data_dirs = glob.glob(os.path.join(data_dirs_path1, \"*.parquet\")) +\\\n",
    "    glob.glob(os.path.join(data_dirs_path2, \"*.parquet\"))\n",
    "\n",
    "target_dirs = glob.glob(os.path.join(target_dirs_path, \"*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530899f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fine-tuning at 2025-08-30 23:41:40.751307\n",
      "Using device: cuda\n",
      "----------------------------------------\n",
      "DataLoader is ready.\n",
      "----------------------------------------\n",
      "Creating model, loss, and optimizer...\n",
      "Model, loss, and optimizer are ready.\n",
      "----------------------------------------\n",
      "Starting fine-tuning loop...\n",
      "\n",
      "Epoch 1/50\n",
      "--------------------\n",
      "Batch 0000 | Loss: 0.7796\n",
      "[Batch 000020] Current Avg Loss: 0.6322\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_20.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000040] Current Avg Loss: 0.6009\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_40.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0050 | Loss: 0.5696\n",
      "[Batch 000060] Current Avg Loss: 0.5855\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_60.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000080] Current Avg Loss: 0.5693\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_80.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000100] Current Avg Loss: 0.5516\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_100.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0100 | Loss: 0.5805\n",
      "Periodic memory clear (batch 100).\n",
      "[Batch 000120] Current Avg Loss: 0.5452\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_120.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000140] Current Avg Loss: 0.5366\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_140.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0150 | Loss: 0.5924\n",
      "[Batch 000160] Current Avg Loss: 0.5411\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_160.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000180] Current Avg Loss: 0.5341\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_180.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000200] Current Avg Loss: 0.5272\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_200.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0200 | Loss: 0.4032\n",
      "Periodic memory clear (batch 200).\n",
      "[Batch 000220] Current Avg Loss: 0.5189\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_220.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000240] Current Avg Loss: 0.5118\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_240.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0250 | Loss: 0.4622\n",
      "[Batch 000260] Current Avg Loss: 0.5092\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_260.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000280] Current Avg Loss: 0.5058\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_280.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000300] Current Avg Loss: 0.4999\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_300.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0300 | Loss: 0.4397\n",
      "Periodic memory clear (batch 300).\n",
      "[Batch 000320] Current Avg Loss: 0.4963\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_320.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000340] Current Avg Loss: 0.4928\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_340.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0350 | Loss: 0.5566\n",
      "[Batch 000360] Current Avg Loss: 0.4916\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_360.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000380] Current Avg Loss: 0.4881\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_380.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000400] Current Avg Loss: 0.4875\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_400.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0400 | Loss: 0.3104\n",
      "Periodic memory clear (batch 400).\n",
      "[Batch 000420] Current Avg Loss: 0.4863\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_420.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000440] Current Avg Loss: 0.4848\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_440.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0450 | Loss: 0.3962\n",
      "[Batch 000460] Current Avg Loss: 0.4821\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_460.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000480] Current Avg Loss: 0.4807\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_480.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000500] Current Avg Loss: 0.4786\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_500.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0500 | Loss: 0.4434\n",
      "Periodic memory clear (batch 500).\n",
      "[Batch 000520] Current Avg Loss: 0.4766\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_520.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000540] Current Avg Loss: 0.4752\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_540.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0550 | Loss: 0.4545\n",
      "[Batch 000560] Current Avg Loss: 0.4715\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_560.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000580] Current Avg Loss: 0.4690\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_580.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000600] Current Avg Loss: 0.4666\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_600.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0600 | Loss: 0.2804\n",
      "Periodic memory clear (batch 600).\n",
      "[Batch 000620] Current Avg Loss: 0.4643\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_620.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000640] Current Avg Loss: 0.4633\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_640.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0650 | Loss: 0.5548\n",
      "[Batch 000660] Current Avg Loss: 0.4620\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_660.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000680] Current Avg Loss: 0.4612\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_680.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000700] Current Avg Loss: 0.4595\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_700.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0700 | Loss: 0.3207\n",
      "Periodic memory clear (batch 700).\n",
      "[Batch 000720] Current Avg Loss: 0.4585\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_720.pth\n",
      "Memory cleared after checkpoint.\n",
      "Epoch 1 Summary:\n",
      "  Avg Loss: 0.4575\n",
      "  Time: 403.97 min\n",
      "  Current LR: 1.00e-03\n",
      "New best fine-tune loss: 0.4575\n",
      "Memory cleared at the end of epoch 1.\n",
      "\n",
      "Epoch 2/50\n",
      "--------------------\n",
      "Batch 0000 | Loss: 0.3446\n",
      "[Batch 000740] Current Avg Loss: 0.4185\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_740.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000760] Current Avg Loss: 0.3869\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_760.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000780] Current Avg Loss: 0.3920\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_780.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0050 | Loss: 0.3583\n",
      "[Batch 000800] Current Avg Loss: 0.3935\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_800.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000820] Current Avg Loss: 0.3961\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_820.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0100 | Loss: 0.3487\n",
      "Periodic memory clear (batch 100).\n",
      "[Batch 000840] Current Avg Loss: 0.3991\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_840.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000860] Current Avg Loss: 0.3946\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_860.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000880] Current Avg Loss: 0.3965\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_880.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0150 | Loss: 0.3632\n",
      "[Batch 000900] Current Avg Loss: 0.4025\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_900.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000920] Current Avg Loss: 0.4032\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_920.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0200 | Loss: 0.4448\n",
      "Periodic memory clear (batch 200).\n",
      "[Batch 000940] Current Avg Loss: 0.4031\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_940.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000960] Current Avg Loss: 0.4012\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_960.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 000980] Current Avg Loss: 0.3999\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_980.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0250 | Loss: 0.3696\n",
      "[Batch 001000] Current Avg Loss: 0.3978\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1000.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 001020] Current Avg Loss: 0.3962\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1020.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0300 | Loss: 0.4643\n",
      "Periodic memory clear (batch 300).\n",
      "[Batch 001040] Current Avg Loss: 0.3976\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1040.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 001060] Current Avg Loss: 0.3973\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1060.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 001080] Current Avg Loss: 0.3963\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1080.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0350 | Loss: 0.2720\n",
      "[Batch 001100] Current Avg Loss: 0.3946\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1100.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 001120] Current Avg Loss: 0.3933\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1120.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0400 | Loss: 0.4047\n",
      "Periodic memory clear (batch 400).\n",
      "[Batch 001140] Current Avg Loss: 0.3949\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1140.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 001160] Current Avg Loss: 0.3949\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1160.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 001180] Current Avg Loss: 0.3946\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1180.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0450 | Loss: 0.4151\n",
      "[Batch 001200] Current Avg Loss: 0.3935\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1200.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 001220] Current Avg Loss: 0.3945\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1220.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0500 | Loss: 0.4367\n",
      "Periodic memory clear (batch 500).\n",
      "[Batch 001240] Current Avg Loss: 0.3954\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1240.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 001260] Current Avg Loss: 0.3947\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1260.pth\n",
      "Memory cleared after checkpoint.\n",
      "[Batch 001280] Current Avg Loss: 0.3940\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1280.pth\n",
      "Memory cleared after checkpoint.\n",
      "Batch 0550 | Loss: 0.4010\n",
      "[Batch 001300] Current Avg Loss: 0.3933\n",
      "Current LR: 1.00e-03\n",
      "Fine-tune checkpoint saved to ./checkpoints/finetune_checkpoint_batch_1300.pth\n",
      "Memory cleared after checkpoint.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting fine-tuning at {datetime.now()}\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "dataset = UserHistoryDataset(\n",
    "    data_dirs, \n",
    "    target_dirs, \n",
    "    succ_pred,\n",
    "    fail_pred,\n",
    "    state_map,\n",
    "    window_size=500,\n",
    "    stride=250\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=final_collate_fn,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False\n",
    ")\n",
    "print(\"DataLoader is ready.\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"Creating model, loss, and optimizer...\")\n",
    "ITEM_INPUT_DIM = 128       \n",
    "USER_FEATURE_DIM = 13      \n",
    "ITEM_OUTPUT_DIM = 128\n",
    "USER_OUTPUT_DIM = 128\n",
    "\n",
    "model = TwoTowerModel(\n",
    "    item_input_dim=ITEM_INPUT_DIM,\n",
    "    item_output_dim=ITEM_OUTPUT_DIM,\n",
    "    user_feature_dim=USER_FEATURE_DIM,\n",
    "    gru_hidden_dim=64,\n",
    "    user_output_dim=USER_OUTPUT_DIM,\n",
    "    mlp_hidden_dim=128,\n",
    "    dropout_rate=0.2\n",
    ").to(DEVICE)\n",
    "\n",
    "FINE_TUNE_LR = 5e-5\n",
    "criterion = PairwiseLogisticLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=FINE_TUNE_LR, weight_decay=1e-6)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=800, gamma=0.9)\n",
    " \n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "print(\"Model, loss, and optimizer are ready.\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"Starting fine-tuning loop...\")\n",
    "best_loss = float('inf')\n",
    "total_batches_processed = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        try:\n",
    "            padded_features_tuple, masks_tuple, history_item_embs_tuple, pos_items_tuple, neg_items_tuple = batch\n",
    "            \n",
    "            padded_features_tensor = torch.tensor(np.stack(padded_features_tuple), dtype=torch.float32)\n",
    "            attention_masks_tensor = torch.tensor(np.stack(masks_tuple), dtype=torch.float32)\n",
    "\n",
    "            padded_features_tensor = padded_features_tensor.to(DEVICE)\n",
    "            attention_masks_tensor = attention_masks_tensor.to(DEVICE)\n",
    "            \n",
    "            history_item_embs_tensors = []\n",
    "            for emb_list in history_item_embs_tuple:\n",
    "                if len(emb_list) > 0:\n",
    "                    if isinstance(emb_list[0], np.ndarray):\n",
    "                        emb_array = np.array(emb_list)\n",
    "                    else:\n",
    "                        emb_array = np.array([emb_list]) if np.isscalar(emb_list[0]) else np.array(emb_list)\n",
    "                    emb_tensor = torch.tensor(emb_array, dtype=torch.float32).to(DEVICE)\n",
    "                else:\n",
    "                    emb_tensor = torch.zeros((1, ITEM_INPUT_DIM), dtype=torch.float32, device=DEVICE)\n",
    "                history_item_embs_tensors.append(emb_tensor)\n",
    "            \n",
    "            pos_items_tensors = []\n",
    "            for emb_list in pos_items_tuple:\n",
    "                if len(emb_list) > 0:\n",
    "                    if isinstance(emb_list[0], np.ndarray):\n",
    "                        emb_array = np.array(emb_list)\n",
    "                    else:\n",
    "                        emb_array = np.array([emb_list]) if np.isscalar(emb_list[0]) else np.array(emb_list)\n",
    "                    emb_tensor = torch.tensor(emb_array, dtype=torch.float32).to(DEVICE)\n",
    "                else:\n",
    "                    emb_tensor = torch.zeros((0, ITEM_INPUT_DIM), dtype=torch.float32, device=DEVICE)\n",
    "                pos_items_tensors.append(emb_tensor)\n",
    "            \n",
    "            neg_items_tensors = []\n",
    "            for emb_list in neg_items_tuple:\n",
    "                if len(emb_list) > 0:\n",
    "                    if isinstance(emb_list[0], np.ndarray):\n",
    "                        emb_array = np.array(emb_list)\n",
    "                    else:\n",
    "                        emb_array = np.array([emb_list]) if np.isscalar(emb_list[0]) else np.array(emb_list)\n",
    "                    emb_tensor = torch.tensor(emb_array, dtype=torch.float32).to(DEVICE)\n",
    "                else:\n",
    "                    emb_tensor = torch.zeros((0, ITEM_INPUT_DIM), dtype=torch.float32, device=DEVICE)\n",
    "                neg_items_tensors.append(emb_tensor)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pos_scores, neg_scores = model(\n",
    "                padded_features_tensor,\n",
    "                attention_masks_tensor,\n",
    "                history_item_embs_tensors,\n",
    "                pos_items_tensors,\n",
    "                neg_items_tensors\n",
    "            )\n",
    "            \n",
    "            loss = criterion(pos_scores, neg_scores)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            total_batches_processed += 1\n",
    "\n",
    "            LOG_INTERVAL = 50\n",
    "            SAVE_INTERVAL = 20\n",
    "            \n",
    "            if batch_idx % LOG_INTERVAL == 0:\n",
    "                print(f\"Batch {batch_idx:04d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "            if total_batches_processed % SAVE_INTERVAL == 0:\n",
    "                current_avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "                print(f\"[Batch {total_batches_processed:06d}] Current Avg Loss: {current_avg_loss:.4f}\")\n",
    "                print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "                \n",
    "                os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "                checkpoint_path = os.path.join(SAVE_PATH, f\"finetune_checkpoint_batch_{total_batches_processed}.pth\")\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'loss': current_avg_loss,\n",
    "                    'batch_num': total_batches_processed,\n",
    "                    'epoch': epoch,\n",
    "                    'lr': optimizer.param_groups[0]['lr']\n",
    "                }, checkpoint_path)\n",
    "                print(f\"Fine-tune checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "                del pos_scores, neg_scores, loss\n",
    "                if DEVICE.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                print(f\"Memory cleared after checkpoint.\")\n",
    "\n",
    "            MEMORY_CLEAR_INTERVAL = 100\n",
    "            if batch_idx % MEMORY_CLEAR_INTERVAL == 0 and batch_idx > 0:\n",
    "                if DEVICE.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                print(f\"Periodic memory clear (batch {batch_idx}).\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            if DEVICE.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            print(f\"Memory cleared after error in batch {batch_idx}.\")\n",
    "            continue\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  Avg Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Time: {epoch_time/60:.2f} min\")\n",
    "    print(f\"  Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(SAVE_PATH, f\"best_finetuned_model_epoch_{epoch+1}_loss_{avg_loss:.4f}.pth\"))\n",
    "        print(f\"New best fine-tune loss: {best_loss:.4f}\")\n",
    "\n",
    "    if DEVICE.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"Memory cleared at the end of epoch {epoch+1}.\")\n",
    "\n",
    "print(\"\\nFine-tuning finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6233351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8093504,
     "sourceId": 12800757,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8130313,
     "sourceId": 12854265,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8139768,
     "sourceId": 12868008,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8143044,
     "sourceId": 12874235,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8144144,
     "sourceId": 12874255,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8158916,
     "sourceId": 12895259,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8155823,
     "sourceId": 12895583,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 438580,
     "modelInstanceId": 420956,
     "sourceId": 551631,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 439202,
     "modelInstanceId": 421592,
     "sourceId": 552869,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "gemorr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-30T23:40:49.475398",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
